# if (!requireNamespace("BiocManager", quietly = TRUE))
#    install.packages("BiocManager")
# BiocManager::install("dada2", version = "3.20")

library(dada2)

packageVersion("dada2")

# download sequencing data
path <- "/Users/veronicagosnell/Desktop/Computational_Genomics/dada2/data_set/MiSeq_SOP"
list.files(path)
# These fastq files were generated by 2x250 Illumina Miseq amplicon sequencing
# of the V4 region of the 16S rRNA gene from gut samples collected longitudinally
# from a mouse post-weaning. For now just consider them paired-end fastq files to be processed.


# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
# Seperate the forward and reverse file into two variable fnFs and fnRs
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
# need sort, if not the vectors may not match each other



# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

# as.vector(sample.names)

# Inspect read quality profiles
# Start visualizing the quality profiles for the forward reads
plotQualityProfile(fnFs[1:2])


# In gray-scale is a heat map of the frequency of each quality score at each base position.
# The mean quality score at each position is shown by the green line,
# and the quartiles of the quality score distribution by the orange lines.
# The red line shows the scaled proportion of reads that extend to at least that position
# (this is more useful for other sequencing technologies, as Illumina reads are typically all the same length,
#   hence the flat red line).
# The forward reads are good quality.
# We generally advise trimming the last few nucleotides to avoid less
# well-controlled errors that can arise there. These quality profiles do not suggest
# that any additional trimming is needed. We will truncate the forward reads
# at position 240 (trimming the last 10 nucleotides).


# Now we visualize the quality profile of the reverse reads:
plotQualityProfile(fnRs[1:2])


# The reverse reads are of significantly worse quality,
# especially at the end, which is common in Illumina sequencing.
# This isn’t too worrisome, as DADA2 incorporates quality information into its error model
# which makes the algorithm robust to lower quality sequence,
# but trimming as the average qualities crash will improve the algorithm’s sensitivity to rare sequence variants.
# Based on these profiles, we will truncate the reverse reads at position 160 where the quality distribution crashes.

# Filter and trim
# Assign the filenames for the filtered fastq.gz files.

# Place filtered files in filtered/ subdirectory
# build new filtered folder, add trimmed reads to it
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names


# use standard filtering parameters: maxN=0 (DADA2 requires no Ns), truncQ=2, rm.phix=TRUE and maxEE=2
# The maxEE parameter sets the maximum number of “expected errors” allowed in a read

# Place filtered files in filtered/ subdirectory
out <- filterAndTrim(fnFs,   # paths to forward fastq files
                     filtFs, # paths to the output forward filtered files
                     fnRs,   # paths to reverse fastq files
                     filtRs, # paths to output reverse filtered files
                     truncLen=c(240, 160),# Truncate the bases after 240 in forward sequences and after 160 in reverse sequences; reads shorter than this are discarded
                     maxN=0, # after truncation, sequences with more than '0' will be discarded
                     maxEE=2, # after truncation, reads with higher than maxEE "expected errors" will be discarded
                     truncQ=2,# after truncation, reads contain a quality score < '2' will be discarded
                     rm.phix=TRUE, # discard reads that match against the phiX genome
                     compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)
# how many reads were trimmmed off


# Learn the error rate
# The DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates.
# The learnErrors method learns this error model from the data, by alternating estimation of the
# error rates and inference of sample composition until they converge on a jointly consistent solution.
# As in many machine-learning problems, the algorithm must begin with an initial guess,
# for which the maximum possible error rates in this data are used
# (the error rates if only the most abundant sequence is correct and all the rest are errors).

#learn the error rate for forward reads(16*41 matrix)
# 41 is cresponding to the Quality value
errF <- learnErrors(filtFs, multithread=TRUE)

#learn the error rate for reverse reads
errR <- learnErrors(filtRs, multithread=TRUE)

#plot the results
plotErrors(errF, nominalQ=TRUE)


# Sample inference
# Apply the core sample inference algorithm to the filtered and trimmed sequence data
# further trim reads

dadaFs <- dada(filtFs, err=errF, multithread=TRUE)

dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
# tells how many reads are unique -> aka sequences
# for classification
# sample 20 is mock -> quality control

dadaFs[[1]]
dadaRs[[1]]

# Merge paired reads
# Merge the forward and reverse reads together to obtain the full denoised sequences.
# Merging is performed by aligning the denoised forward reads with the reverse-complement
# of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences.
# By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases,
# and are identical to each other in the overlap region (but these conditions can be changed via function arguments).

mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# filtered and separated for f and r
# reads should now be fairly confident
# need to merge them together

# Inspect the merger data.frame from the first sample
head(mergers[[1]])
# sequencing information -> only sequenced a part of the 16S RNA -> why this section is short
# abundance of each read, etc.
# ex./ 107 is final number of unique reads -> rows in matrix
  # also tells abundance, for each sequence, how many reads got mapped back to that sequence


# The mergers object is a list of data.frames from each sample.
# Each data.frame contains the merged $sequence, its $abundance, and the indices
# of the $forward and $reverse sequence variants that were merged.
# Paired reads that did not exactly overlap were removed by mergePairs,
# further reducing spurious output.

# Construct ASV sequence table
# Sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# 293 is consensus sequences you have, 19 samples, 1 is mock

# The sequence table is a matrix with rows corresponding to (and named by) the samples,
# and columns corresponding to (and named by) the sequence variants.
# This table contains 293 ASVs, and the lengths of our merged sequences all fall within the expected range for this V4 amplicon.

# Remove chimeras
# The core dada method corrects substitution and indel errors, but chimeras remain.
# Fortunately, the accuracy of sequence variants after denoising makes identifying
# chimeric ASVs simpler than when dealing with fuzzy OTUs.
# Chimeric sequences are identified if they can be exactly reconstructed
# by combining a left-segment and a right-segment from two more abundant “parent” sequences.

# one sequence mapped to multiple genomes
# how is the consensus sequences treated -> similar, but still remove these

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
# Identified 61 bimeras out of 293 input sequences.
# removed 61 chimeras

dim(seqtab.nochim)
# 232 unique sequences of 20 samples

# Number of reads that made it through each step in the pipeline:
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))

# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
# how many reads were kept, and removed
# merged requires number of nucleotides in F and R to have overlap
  # number constantly going down
# too close of a sequence needs to be removed



# Assign taxonomy by naive Bayesian classifier
# The DADA2 package provides a native implementation of the naive Bayesian classifier method
# for this purpose. The assignTaxonomy function takes as input a set of sequences to be classified
# and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments
# with at least minBoot bootstrap confidence.

# 232 sequences, need to assign a bacteria to each sequence
?assignTaxonomy
# use paper linked in man page
# use machine learning naive Bayesian classifier

taxa <- assignTaxonomy(seqtab.nochim, "/Users/veronicagosnell/Desktop/Computational_Genomics/dada2/data_set/silva_nr99_v138_wSpecies_train_set.fa.gz", multithread=TRUE)
# input is the matrix, 20 232
# bacterial genome for reference
# multithread uses parallelism
# minBoot -> test -> what is bootstrap meaning?

# Inspect the taxonomic assignments:
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)


# Evaluate accuracy
# One of the samples included here was a “mock community”,
# in which a mixture of 20 known strains was sequenced.
# Reference sequences corresponding to these strains were provided
# in the downloaded zip archive. We return to that sample and compare
# the sequence variants inferred by DADA2 to the expected composition of the community.

unqs.mock <- seqtab.nochim["Mock",]
unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) # Drop ASVs absent in the Mock
cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock community.\n")
# measure whether the mock matches your reads

mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
match.ref <- sum(sapply(names(unqs.mock), function(x) any(grepl(x, mock.ref))))
cat("Of those,", sum(match.ref), "were exact matches to the expected reference sequences.\n")


# How to calculate Shannon diversity
samples.out <- rownames(seqtab.nochim)
subject <- sapply(strsplit(samples.out, "D"), `[`, 1)
gender  <- substr(subject,1,1)
subject <- substr(subject,2,999)
day     <- as.integer(sapply(strsplit(samples.out, "D"), `[`, 2))
samdf   <- data.frame(Subject=subject, Gender=gender, Day=day)
samdf$When <- "Early"
samdf$When[samdf$Day>100] <- "Late"
rownames(samdf) <- samples.out
samdf$col <- rep('red', nrow(samdf))
samdf$col[samdf$When == 'Early'] <- 'navy'



# Shannon and Simpson diversity
#input should be samples x taxonomy
shannonDiversity <- function(df)
{
  p <- t( apply(df, 1, function(x){x/sum(x)}) ) #normalize
  H <- apply(p , 1, function(x){x <- x[x > 0]; -sum( log(x) * x )}) # calculate shannon diversity
  return(H)
}

SimpsonDiversity <- function(df)
{
  p <- t( apply(df, 1, function(x){x/sum(x)}) ) # normalize
  H <- apply(p , 1, function(x){x <- x[x > 0];1-sum( (x ^ 2))})
  return(H)
}
shD <- shannonDiversity(seqtab.nochim)
siD <- SimpsonDiversity(seqtab.nochim)
par(mfrow = c(1,2))
plot(samdf$Day,shD, col = samdf$col, pch = 19 ,las = 1,
     xlab = 'Days', ylab = 'Shannon Diversity')
plot(samdf$Day,siD, col = samdf$col, pch = 19 ,las = 1,
     xlab = 'Days', ylab = 'Simpson Diversity')


# transform data to proportions as appropriate for Bray-Curtis distance
ps.prop <- t(apply(seqtab.nochim,1,function(x){x/sum(x)}))


# Performing NMDS
# install package vegan
#install.packages('vegan')
library(vegan)


NMDS <- metaMDS(ps.prop[-20,],     # remove mock line
                distance = 'bray') # The number of reduced dimensions


plot(NMDS$points[,1], NMDS$points[,2], col = samdf$col,
     pch = 19, las = 1, xlab = 'MDS1', ylab = 'MDS2')

# barplot shows the taxonomic distribution of top 20 taxonomic
# using ecology distance instead of genome distance?
# difference between PCA and metaMDS

dim(taxa)

dim(seqtab.nochim)


idTop20   <- order(colSums(seqtab.nochim), decreasing = T)[1:20]
top20     <- ps.prop[-nrow(seqtab.nochim),idTop20] #remove Mock
top20Taxa <- data.frame( taxa[idTop20, ] )

#encode color for top20Taxa$Family
library("RColorBrewer")
colNeed <- data.frame(1:9, brewer.pal(9, "Set1"))
top20Taxa$category <- as.numeric(as.factor(top20Taxa$Family))
top20Taxa$category[is.na(top20Taxa$category)] <- 7

top20Taxa$color    <- colNeed[match(top20Taxa$category, colNeed[,1]),2]
samdfUse <- samdf[-nrow(samdf),]
top20prop <- top20[, order(top20Taxa$Family, decreasing = T)]
top20Taxa <- top20Taxa[order(top20Taxa$Family,decreasing = T), ]

#pdf('~/Desktop/aa.pdf', height = 8,width = 10)
par(mfrow = c(1,3))
barplot( t(top20prop[samdfUse$When == 'Early',]), col = top20Taxa$color,
         las = 2)
barplot( t(top20prop[samdfUse$When == 'Late',]), col = top20Taxa$color,
         las = 2)

##for legend
legendMatrix <- unique(cbind(top20Taxa$Family,
                             top20Taxa$color))
par(mar = c(0,0,0,0))
plot(NA,NA, xlab = '', ylab = '',
     xaxt = 'n', yaxt = 'n',xlim = c(0,1), ylim = c(0,1),
     bty = 'n')
legend('center', legend = legendMatrix[,1],
       col = legendMatrix[,2], pch = 15,
       bty = 'n')



#dev.off()

